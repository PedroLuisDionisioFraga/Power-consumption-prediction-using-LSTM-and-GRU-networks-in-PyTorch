{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso usar o colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# data_dir = \"/content/drive/MyDrive/Meus coÃÅdigos/data\"\n",
    "# print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso usar localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/After Filters\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_sliding_window(data, window_size, inputs_cols_indices, label_col_index):\n",
    "    \"\"\"\n",
    "    data: numpy array including data\n",
    "    window_size: size of window\n",
    "    inputs_cols_indices: col indices to include\n",
    "    \"\"\"\n",
    "\n",
    "    # (# instances created by movement, seq_len (timestamps), # features (input_len))\n",
    "    inputs = np.zeros((len(data) - window_size, window_size, len(inputs_cols_indices)))\n",
    "    labels = np.zeros(len(data) - window_size)\n",
    "\n",
    "    for i in range(window_size, len(data)):\n",
    "        inputs[i - window_size] = data[i - window_size : i, inputs_cols_indices]\n",
    "        labels[i - window_size] = data[i, label_col_index]\n",
    "    inputs = inputs.reshape(-1, window_size, len(inputs_cols_indices))\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    print(inputs.shape, labels.shape)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col_index = 0  # consumption as label to predict\n",
    "inputs_cols_indices = range(5)  # use (consumption, hour, dayofweek, month, dayofyear) columns as features\n",
    "\n",
    "# Define window_size period and split inputs/labels\n",
    "window_size = 90\n",
    "\n",
    "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "# Skipping the files we're not using\n",
    "processing_files = [file for file in os.listdir(data_dir) if os.path.splitext(file)[1] == \".csv\"]\n",
    "\n",
    "num_files_for_dataset = 5\n",
    "\n",
    "for file in tqdm_notebook(processing_files[:num_files_for_dataset]):\n",
    "  print(f\"Processing {file} ...\")\n",
    "  # Store csv file in a Pandas DataFrame\n",
    "  df = pd.read_csv(os.path.join(data_dir, file), parse_dates=[\"Datetime\"])\n",
    "  print(df)\n",
    "\n",
    "  # Processing the time data into suitable input formats\n",
    "  df[\"hour\"] = df.apply(lambda x: x[\"Datetime\"].hour, axis=1)\n",
    "  df[\"dayofweek\"] = df.apply(lambda x: x[\"Datetime\"].dayofweek, axis=1)\n",
    "  df[\"month\"] = df.apply(lambda x: x[\"Datetime\"].month, axis=1)\n",
    "  df[\"dayofyear\"] = df.apply(lambda x: x[\"Datetime\"].dayofyear, axis=1)\n",
    "  df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
    "  # df = df.drop(\"Time\", axis=1)\n",
    "\n",
    "  # Scaling the input data\n",
    "  sc = MinMaxScaler()\n",
    "  label_sc = MinMaxScaler()\n",
    "  data = sc.fit_transform(df.values)\n",
    "\n",
    "  # Obtaining the scaler for the labels(usage data) so that output can be\n",
    "  # re-scaled to actual value during evaluation\n",
    "  label_sc.fit(df.iloc[:, label_col_index].values.reshape(-1, 1))\n",
    "  label_scalers[file] = label_sc\n",
    "\n",
    "  # Move the window\n",
    "  inputs, labels = move_sliding_window(\n",
    "    data,\n",
    "    window_size,\n",
    "    inputs_cols_indices=inputs_cols_indices,\n",
    "    label_col_index=label_col_index,\n",
    "  )\n",
    "\n",
    "  # CONCAT created instances from all .csv files.\n",
    "  # Split data into train/test portions and combining all data from different files into a single array\n",
    "  test_portion = int(0.1 * len(inputs))\n",
    "  if len(train_x) == 0:  # first iteration\n",
    "    train_x = inputs[:-test_portion]\n",
    "    train_y = labels[:-test_portion]\n",
    "  else:\n",
    "    train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
    "    train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
    "  test_x[file] = inputs[-test_portion:]\n",
    "  test_y[file] = labels[-test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "\n",
    "# Drop the last incomplete batch\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Size: {train_x.shape}, Batch Size: {batch_size}, # of iterations per epoch: {int(train_x.shape[0]/batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release some memory\n",
    "del train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "    super(GRUNet, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.gru = nn.GRU(\n",
    "      input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob\n",
    "    )\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    out, h = self.gru(x, h)\n",
    "    # print(out[:, -1].shape, h.shape)\n",
    "    # select hidden state of last timestamp (t=90) (1024, 256)\n",
    "    out = self.fc(self.relu(out[:, -1]))  # out[:, -1, :]\n",
    "    # print(out.shape) # (1024, 1)\n",
    "    return out, h\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    # Initialze h_0 with zeros\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (\n",
    "        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "    )\n",
    "    return hidden\n",
    "\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
    "    super(LSTMNet, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    out, h = self.lstm(x, h)\n",
    "    out = self.fc(self.relu(out[:, -1]))\n",
    "    return out, h\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "    # Initialze h_0, c_0 with zeros\n",
    "    hidden = (\n",
    "      weight.new(self.n_layers, batch_size, self.hidden_dim)\n",
    "      .zero_()\n",
    "      .to(device),  # h_0\n",
    "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "    )\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "  train_loader,\n",
    "  learn_rate,\n",
    "  hidden_dim=256,\n",
    "  n_layers=2,\n",
    "  n_epochs=5,\n",
    "  model_type=\"GRU\",\n",
    "  print_every=100,\n",
    "):\n",
    "\n",
    "  input_dim = next(iter(train_loader))[0].shape[2]  # 5\n",
    "\n",
    "  # Batch generator (train_data, train_label)\n",
    "  # print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape) # torch.Size([1024, 90, 5]) torch.Size([1024, 1])\n",
    "\n",
    "  output_dim = 1\n",
    "\n",
    "  # Instantiating the models\n",
    "  if model_type == \"GRU\":\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "  else:\n",
    "    model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "  model.to(device)\n",
    "\n",
    "  # Defining loss function and optimizer\n",
    "  criterion = nn.MSELoss()  # Mean Squared Error\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "  model.train()\n",
    "  print(\"Starting Training of {} model\".format(model_type))\n",
    "  epoch_times = []\n",
    "\n",
    "  # Start training loop\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.process_time()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    avg_loss = 0.0\n",
    "    counter = 0\n",
    "    for x, label in train_loader:\n",
    "      counter += 1\n",
    "      if model_type == \"GRU\":\n",
    "        h = h.data\n",
    "      # Unpcak both h_0 and c_0\n",
    "      elif model_type == \"LSTM\":\n",
    "        h = tuple([e.data for e in h])\n",
    "\n",
    "      # Set the gradients to zero before starting to do backpropragation because\n",
    "      # PyTorch accumulates the gradients on subsequent backward passes\n",
    "      model.zero_grad()\n",
    "\n",
    "      out, h = model(x.to(device).float(), h)\n",
    "      loss = criterion(out, label.to(device).float())\n",
    "\n",
    "      # Perform backpropragation\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      avg_loss += loss.item()\n",
    "      if counter % print_every == 0:\n",
    "        print(f\"Epoch {epoch}/{n_epochs} - Step: {counter}/{len(train_loader)} - Average Loss for Epoch: {avg_loss/counter}\")\n",
    "    current_time = time.process_time()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{n_epochs} Done, Total Loss: {avg_loss/len(train_loader)}\")\n",
    "\n",
    "    print(f\"Time Elapsed for Epoch: {current_time-start_time} seconds\")\n",
    "\n",
    "    epoch_times.append(current_time - start_time)\n",
    "\n",
    "  print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = 90  # (timestamps)\n",
    "n_hidden = 256\n",
    "n_layers = 2\n",
    "n_epochs = 4\n",
    "print_every = 5\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = train(\n",
    "  train_loader,\n",
    "  learn_rate=lr,\n",
    "  hidden_dim=n_hidden,\n",
    "  n_layers=n_layers,\n",
    "  n_epochs=n_epochs,\n",
    "  model_type=\"GRU\",\n",
    "  print_every=print_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso usar o colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(gru_model.state_dict(), \"/content/drive/MyDrive/Meus coÃÅdigos/models/gru_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso usar localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model.state_dict(), \"./models/gru_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = train(\n",
    "  train_loader,\n",
    "  learn_rate=lr,\n",
    "  hidden_dim=n_hidden,\n",
    "  n_layers=n_layers,\n",
    "  n_epochs=n_epochs,\n",
    "  model_type=\"LSTM\",\n",
    "  print_every=print_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso usar o colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(gru_model.state_dict(), \"/content/drive/MyDrive/Meus coÃÅdigos/models/lstm_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso usar localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), \"./models/lstm_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move device to cpu for evaluation to avoid GPU memory run\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 5\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "gru_model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "gru_model.load_state_dict(torch.load(\"./models/gru_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device\n",
    "gru_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256\n",
    "input_dim = 5\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "lstm_model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "lstm_model.load_state_dict(torch.load(\"./models/lstm_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sMAPE(outputs, targets):\n",
    "  sMAPE = (\n",
    "    100\n",
    "    / len(targets)\n",
    "    * np.sum(np.abs(outputs - targets) / (np.abs(outputs + targets)) / 2)\n",
    "  )\n",
    "  return sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y, label_scalers):\n",
    "  model.eval()\n",
    "  outputs = []\n",
    "  targets = []\n",
    "  start_time = time.process_time()\n",
    "  # get data of test data for each state\n",
    "  for file in test_x.keys():\n",
    "    inputs = torch.from_numpy(np.array(test_x[file]))\n",
    "    labels = torch.from_numpy(np.array(test_y[file]))\n",
    "\n",
    "    h = model.init_hidden(inputs.shape[0])\n",
    "\n",
    "    # predict outputs\n",
    "    with torch.no_grad():\n",
    "      out, h = model(inputs.to(device).float(), h)\n",
    "\n",
    "    outputs.append(\n",
    "      label_scalers[file]\n",
    "      .inverse_transform(out.cpu().detach().numpy())\n",
    "      .reshape(-1)\n",
    "    )\n",
    "\n",
    "    targets.append(label_scalers[file].inverse_transform(labels.numpy()).reshape(-1))\n",
    "\n",
    "  # Merge all files\n",
    "  concatenated_outputs = np.concatenate(outputs)\n",
    "  concatenated_targets = np.concatenate(targets)\n",
    "\n",
    "  print(f\"Evaluation Time: {time.process_time()-start_time}\")\n",
    "  print(f\"sMAPE: {round(sMAPE(concatenated_outputs, concatenated_targets), 3)}%\")\n",
    "\n",
    "  # list of of targets/outputs for each state\n",
    "  return outputs, targets, sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outputs, targets, lstm_sMAPE = evaluate(lstm_model, test_x, test_y, label_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gru_outputs)  # list of predicted output file for each state (each element has a 1d array for that state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_list = list(test_x.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alguns m√©todos explicados\n",
    "1) *`gca()`* √© uma abrevia√ß√£o de \"get current axes\" (obter eixos atuais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rangeAbscissaAxisToOutput1 = 25\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "outputGRU = gru_outputs[0][-rangeAbscissaAxisToOutput1:]\n",
    "\n",
    "#* Output 1\n",
    "plt.plot(outputGRU, \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "plt.plot(lstm_outputs[0][-rangeAbscissaAxisToOutput1:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "plt.plot(targets[0][-rangeAbscissaAxisToOutput1:], color=\"b\", label=\"Actual\")\n",
    "\n",
    "plt.yticks(np.arange(10, 180 + 1, 10))\n",
    "plt.xticks(np.arange(0, rangeAbscissaAxisToOutput1 + 1, 1))\n",
    "plt.grid(True, color='gray', linestyle='dashed', linewidth=0.8)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Unidade de medida dos eixos\n",
    "plt.ylabel(\"Energy Consumption (MW)\")\n",
    "plt.xlabel(\"Hours\")  # Eu, Pedro Luis, criei isso pra por no artigo\n",
    "plt.title(f\"Energy Consumption for {states_list[0]} state\")\n",
    "\n",
    "plt.yticks(np.arange(10, 180 + 1, 10))\n",
    "plt.xticks(np.arange(0, rangeAbscissaAxisToOutput1 + 1, 1))\n",
    "plt.grid(True, color='gray', linestyle='dashed', linewidth=0.8)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#* Output 2\n",
    "\n",
    "# rangeAbscissaAxisToOutput2 = 25\n",
    "\n",
    "# plt.figure(figsize=(18, 14))\n",
    "# plt.subplot(2, 2, 1)\n",
    "\n",
    "# outputGRU = gru_outputs[0][-rangeAbscissaAxisToOutput2:]\n",
    "# plt.plot(gru_outputs[1][-rangeAbscissaAxisToOutput2:], \"-o\", color=\"g\", label=\"GRU Predictions\", markersize=2)\n",
    "# plt.plot(lstm_outputs[1][-rangeAbscissaAxisToOutput2:], \"-o\", color=\"r\", label=\"LSTM Predictions\", markersize=2)\n",
    "# plt.plot(targets[1][-rangeAbscissaAxisToOutput2:], color=\"b\", label=\"Actual\")\n",
    "\n",
    "# plt.ylabel(\"Energy Consumption (MW)\")\n",
    "# plt.xlabel(\"Hours\")  # \n",
    "# plt.title(f\"Energy Consumption for {states_list[1]} state\")\n",
    "\n",
    "# plt.yticks(np.arange(10, 180 + 1, 10))\n",
    "# plt.xticks(np.arange(0, rangeAbscissaAxisToOutput2 + 1, 1))\n",
    "# plt.grid(True, color='gray', linestyle='dashed', linewidth=0.8)\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Personalizar a cor da grade, peguei do gpt, n sei como funciona\n",
    "# plt.gca().yaxis.grid(True, color='gray', linestyle='dashed', linewidth=0.8)\n",
    "# plt.gca().xaxis.grid(True, color='gray', linestyle='dashed', linewidth=0.8)\n",
    "# plt.grid(True, color='gray', linestyle='dashed', linewidth=0.8)\n",
    "\n",
    "# Mostra o gr√°fico\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
